{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5QApBmLhOGP"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "from utils import get_plot\n",
    "from models import GCN\n",
    "from data_process import load_data\n",
    "from train_func import test, Block_matrix_train, Block_matrix_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n0WIOFZPqz2"
   },
   "outputs": [],
   "source": [
    "def get_K_hop_neighbors(adj_matrix, index, K):\n",
    "    adj_matrix = adj_matrix + torch.eye(adj_matrix.shape[0],adj_matrix.shape[1])  #make sure the diagonal part >= 1\n",
    "    hop_neightbor_index=index\n",
    "    for i in range(K):\n",
    "        hop_neightbor_index=torch.unique(torch.nonzero(adj[hop_neightbor_index])[:,1])\n",
    "    return hop_neightbor_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyL-gXSZPqz2"
   },
   "outputs": [],
   "source": [
    "def get_K_hop_neighbors_BDS(adj_matrix, index, K):\n",
    "    adj_matrix = adj_matrix + torch.eye(adj_matrix.shape[0],adj_matrix.shape[1])  #make sure the diagonal part >= 1\n",
    "    \n",
    "    onehop_neightbor_index=torch.unique(torch.nonzero(adj[index])[:,1])\n",
    "    np.setdiff1d(index, onehop_neightbor_index)\n",
    "    \n",
    "    return onehop_neightbor_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \n",
    "    mx = mx + torch.eye(mx.shape[0],mx.shape[1])\n",
    "    \n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return torch.tensor(mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5CMAPT6gafH"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMHZprmsOTdl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def Collaborative_Reasoning(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent):\n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #GCN:n*n\n",
    "        #no connection between agents\n",
    "\n",
    "        #define model\n",
    "\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                    nhid=args_hidden,\n",
    "                    nclass=labels.max().item() + 1,\n",
    "                    dropout=args_dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                    nhid=args_hidden,\n",
    "                    nclass=labels.max().item() + 1,\n",
    "                    dropout=args_dropout))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].to(torch.device('cuda:0'))#.cuda()\n",
    "                global_model=global_model.to(torch.device('cuda:0'))\n",
    "                features = features.cuda()\n",
    "                adj = adj.to(torch.device('cuda:0'))\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # split data into K devices\n",
    "        \n",
    "        n=len(adj)\n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append(np.array(labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        L = []\n",
    "        for i in split_data_indexes:\n",
    "            L += list(i)\n",
    "        L.sort()\n",
    "        iid_indexes = np.setdiff1d(range(len(labels)), L)\n",
    "        \n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "        \n",
    "        \n",
    "        #get train indexes in each device, only part of nodes in each device have labels in the train process\n",
    "        split_train_ids = []\n",
    "        for i in range(K):\n",
    "            split_data_indexes[i].sort()\n",
    "            inter = np.intersect1d(split_data_indexes[i], idx_train)\n",
    "            \n",
    "            split_train_ids.append(np.searchsorted(split_data_indexes[i], inter))   #local id in block matrix\n",
    "            \n",
    "        \n",
    "        \n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        \n",
    "        #start training\n",
    "        for t in range(iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    if len(split_train_ids[i]) == 0:\n",
    "                        continue\n",
    "                    acc_train=Block_matrix_train(epoch, models[i], optimizers[i], features, adj, labels,\n",
    "                                    split_data_indexes[i], split_train_ids[i])\n",
    "                    \n",
    "                acc_trains.append(acc_train)\n",
    "                    #print(model.Lambda)\n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            \n",
    "            \n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = split_train_ids[0].shape[0] * states[0][key]\n",
    "                count_D=split_train_ids[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += split_train_ids[i].shape[0] * states[i][key]\n",
    "                    count_D += split_train_ids[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "            \n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            #print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            #print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_Collaborative_Reasoning_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        #print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_Collaborative_Reasoning_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        #print(\"save file as\",mode+'_'+dataset_name+'_IID_'+str(iid_percent)+'_Collaborative_Reasoning_iter_'+str(iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "\n",
    "\n",
    "        return loss_test, acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubM8c3SqXwA3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "mode=\"real\"\n",
    "dataset_name='cora'\n",
    "features, adj, labels, idx_train, idx_val, idx_test = load_data(dataset_name)\n",
    "class_num = labels.max().item() + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jDXPAhGUu16",
    "outputId": "2bbdd38b-f48d-4338-dd07-41bcce391781",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for fix seed, need to rerun both data and model codes\n",
    "\n",
    "args_normalize = True\n",
    "\n",
    "model_type = 'GCN'    #GCN\n",
    "args_hidden = 16\n",
    "args_dropout = 0.5\n",
    "args_lr = 0.5\n",
    "args_weight_decay = 5e-4     #L2 penalty\n",
    "args_epochs = 3\n",
    "args_no_cuda = False\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "args_device_num = class_num #split data into args_device_num parts\n",
    "#iterations = 100\n",
    "\n",
    "\n",
    "\n",
    "if args_normalize==True:  \n",
    "    adj = normalize(adj)\n",
    "    '''\n",
    "    adj = adj + torch.eye(adj.shape[0],adj.shape[1])\n",
    "    d=torch.sum(adj,axis=1)\n",
    "    D_minus_one_over_2=torch.zeros(adj.shape[0],adj.shape[0])\n",
    "    D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d**(-0.5)\n",
    "    adj = torch.mm(torch.mm(D_minus_one_over_2,adj),D_minus_one_over_2)\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for args_epochs in [3]:\n",
    "    for args_random_assign in [0.0, 0.5, 1]:\n",
    "        for args_device_num in [2,3,4,5,6]:\n",
    "            for iterations in [10,20,40,80,100,200,300,400,500,600]:\n",
    "       # for i in range(3):\n",
    "                Collaborative_Reasoning(args_device_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a410216a586027e41e6693bdeb1563026181c5d1fabea1354baf3177ace6ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
